{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f4db6a-6619-4c01-8a1a-24287c871ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "SEED = 2024\n",
    "random.seed(SEED)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "from imblearn.metrics import specificity_score, sensitivity_score\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.utils import check_array, _safe_indexing, sparsefuncs_fast, check_X_y, check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.base import clone\n",
    "from numbers import Integral\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SVMSMOTE\n",
    "import os\n",
    "# import missingpy as missingpy\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "#from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cbe91a-3b7a-4a43-9195-ed31ada1182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Loan_default.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7c68c4-8fc7-4389-a164-5cdc0f3b416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51940 entries, 0 to 51939\n",
      "Columns: 680 entries, id to loss\n",
      "dtypes: float64(517), int64(162), uint64(1)\n",
      "memory usage: 269.5 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e55d94-19df-49a2-9e1d-21b30e650273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f769</th>\n",
       "      <th>f771</th>\n",
       "      <th>f772</th>\n",
       "      <th>f773</th>\n",
       "      <th>f774</th>\n",
       "      <th>f775</th>\n",
       "      <th>nominal_3</th>\n",
       "      <th>nominal_4</th>\n",
       "      <th>f778</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500080</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>83607</td>\n",
       "      <td>1800</td>\n",
       "      <td>1527</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.18</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>9</td>\n",
       "      <td>0.502749</td>\n",
       "      <td>2900</td>\n",
       "      <td>4</td>\n",
       "      <td>79124</td>\n",
       "      <td>89</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.12</td>\n",
       "      <td>6.11</td>\n",
       "      <td>-3.82</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>-0.5399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "      <td>0.985674</td>\n",
       "      <td>2900</td>\n",
       "      <td>4</td>\n",
       "      <td>13026</td>\n",
       "      <td>4565</td>\n",
       "      <td>263</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.42</td>\n",
       "      <td>7.06</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>-0.6732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>9</td>\n",
       "      <td>0.385778</td>\n",
       "      <td>2900</td>\n",
       "      <td>4</td>\n",
       "      <td>79244</td>\n",
       "      <td>6597</td>\n",
       "      <td>3592</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.11</td>\n",
       "      <td>4.45</td>\n",
       "      <td>-3.26</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>-0.7220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>9</td>\n",
       "      <td>0.745471</td>\n",
       "      <td>2900</td>\n",
       "      <td>4</td>\n",
       "      <td>78920</td>\n",
       "      <td>3058</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>2.02</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.2601</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51935</th>\n",
       "      <td>105464</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>8</td>\n",
       "      <td>0.420418</td>\n",
       "      <td>2200</td>\n",
       "      <td>4</td>\n",
       "      <td>3793</td>\n",
       "      <td>7820</td>\n",
       "      <td>11541</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.81</td>\n",
       "      <td>8.26</td>\n",
       "      <td>-6.08</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51936</th>\n",
       "      <td>105465</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>8</td>\n",
       "      <td>0.839374</td>\n",
       "      <td>2200</td>\n",
       "      <td>4</td>\n",
       "      <td>7458</td>\n",
       "      <td>7548</td>\n",
       "      <td>3440</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>3.53</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>0.7329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51937</th>\n",
       "      <td>105466</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888521</td>\n",
       "      <td>2200</td>\n",
       "      <td>4</td>\n",
       "      <td>9836</td>\n",
       "      <td>6377</td>\n",
       "      <td>4286</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.3367</td>\n",
       "      <td>0.9228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51938</th>\n",
       "      <td>105467</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>8</td>\n",
       "      <td>0.006620</td>\n",
       "      <td>2200</td>\n",
       "      <td>4</td>\n",
       "      <td>76831</td>\n",
       "      <td>331</td>\n",
       "      <td>2366</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.3809</td>\n",
       "      <td>-0.5489</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51939</th>\n",
       "      <td>105469</td>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>9</td>\n",
       "      <td>0.241858</td>\n",
       "      <td>2200</td>\n",
       "      <td>4</td>\n",
       "      <td>627</td>\n",
       "      <td>7081</td>\n",
       "      <td>6577</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.07</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.1649</td>\n",
       "      <td>1.0901</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51940 rows × 680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  loan_status   f1  f2        f3    f4  f5     f6    f7     f8  \\\n",
       "0           3            0  126  10  0.500080  1100   3  83607  1800   1527   \n",
       "1           5            0  109   9  0.502749  2900   4  79124    89    491   \n",
       "2           7            0  121   9  0.985674  2900   4  13026  4565    263   \n",
       "3           8            1  128   9  0.385778  2900   4  79244  6597   3592   \n",
       "4           9            0  126   9  0.745471  2900   4  78920  3058    112   \n",
       "...       ...          ...  ...  ..       ...   ...  ..    ...   ...    ...   \n",
       "51935  105464            1  136   8  0.420418  2200   4   3793  7820  11541   \n",
       "51936  105465            0  127   8  0.839374  2200   4   7458  7548   3440   \n",
       "51937  105466            0  140   8  0.888521  2200   4   9836  6377   4286   \n",
       "51938  105467            0  126   8  0.006620  2200   4  76831   331   2366   \n",
       "51939  105469            0  129   9  0.241858  2200   4    627  7081   6577   \n",
       "\n",
       "       ...   f769  f771  f772  f773    f774    f775  nominal_3  nominal_4  \\\n",
       "0      ...  -5.18  2.89 -1.73  1.04  0.2521  0.7258          1          0   \n",
       "1      ... -11.12  6.11 -3.82  2.51  0.2282 -0.5399          0          0   \n",
       "2      ... -11.42  7.06 -4.99  3.77  0.2458 -0.6732          0          0   \n",
       "3      ...  -7.11  4.45 -3.26  2.56  0.2947 -0.7220          0          0   \n",
       "4      ...  -3.29  2.02 -1.35  0.95  0.2601  0.7132          0          0   \n",
       "...    ...    ...   ...   ...   ...     ...     ...        ...        ...   \n",
       "51935  ... -11.81  8.26 -6.08  4.60  0.2199  0.9908          0          0   \n",
       "51936  ...  -5.34  3.53 -2.54  1.90  0.2605  0.7329          0          0   \n",
       "51937  ...  -1.80  1.42 -1.17  0.98  0.3367  0.9228          0          0   \n",
       "51938  ...  -1.38  1.07 -0.92  0.82  0.3809 -0.5489          0          0   \n",
       "51939  ...  -2.07  1.18 -0.70  0.44  0.1649  1.0901          1          0   \n",
       "\n",
       "       f778  loss  \n",
       "0         5     0  \n",
       "1         5     0  \n",
       "2         5     0  \n",
       "3         5     1  \n",
       "4         5     0  \n",
       "...     ...   ...  \n",
       "51935    93     3  \n",
       "51936    93     0  \n",
       "51937    93     0  \n",
       "51938    93     0  \n",
       "51939    93     0  \n",
       "\n",
       "[51940 rows x 680 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7918e3e-675f-4eea-a871-84c3937e61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(n=6000, random_state=2025).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05574d3-2a62-44b7-953e-f248f1e1b2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "loan_status    0\n",
       "f1             0\n",
       "f2             0\n",
       "f3             0\n",
       "              ..\n",
       "f775           0\n",
       "nominal_3      0\n",
       "nominal_4      0\n",
       "f778           0\n",
       "loss           0\n",
       "Length: 680, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e11f019-d995-4b8f-9ee6-996cdb6a8f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f769</th>\n",
       "      <th>f771</th>\n",
       "      <th>f772</th>\n",
       "      <th>f773</th>\n",
       "      <th>f774</th>\n",
       "      <th>f775</th>\n",
       "      <th>nominal_3</th>\n",
       "      <th>nominal_4</th>\n",
       "      <th>f778</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63501</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>7</td>\n",
       "      <td>0.239081</td>\n",
       "      <td>5100</td>\n",
       "      <td>10</td>\n",
       "      <td>82134</td>\n",
       "      <td>1850</td>\n",
       "      <td>341</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>2.40</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.2833</td>\n",
       "      <td>-0.2578</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14905</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>0.582750</td>\n",
       "      <td>1300</td>\n",
       "      <td>3</td>\n",
       "      <td>510</td>\n",
       "      <td>4223</td>\n",
       "      <td>1463</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.61</td>\n",
       "      <td>2.88</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.2747</td>\n",
       "      <td>0.2589</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10599</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>8</td>\n",
       "      <td>0.934541</td>\n",
       "      <td>2300</td>\n",
       "      <td>4</td>\n",
       "      <td>78252</td>\n",
       "      <td>3841</td>\n",
       "      <td>2813</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.1956</td>\n",
       "      <td>0.4659</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47782</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>10</td>\n",
       "      <td>0.472430</td>\n",
       "      <td>1300</td>\n",
       "      <td>16</td>\n",
       "      <td>86027</td>\n",
       "      <td>3764</td>\n",
       "      <td>5484</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.87</td>\n",
       "      <td>2.51</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.2109</td>\n",
       "      <td>-0.5439</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51763</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>8</td>\n",
       "      <td>0.129895</td>\n",
       "      <td>5600</td>\n",
       "      <td>4</td>\n",
       "      <td>81227</td>\n",
       "      <td>64</td>\n",
       "      <td>1265</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.78</td>\n",
       "      <td>12.15</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>6.56</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.5302</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  loan_status   f1  f2        f3    f4  f5     f6    f7    f8  ...  \\\n",
       "0  63501            0  163   7  0.239081  5100  10  82134  1850   341  ...   \n",
       "1  14905            0  126  10  0.582750  1300   3    510  4223  1463  ...   \n",
       "2  10599            0  122   8  0.934541  2300   4  78252  3841  2813  ...   \n",
       "3  47782            0  118  10  0.472430  1300  16  86027  3764  5484  ...   \n",
       "4  51763            0  153   8  0.129895  5600   4  81227    64  1265  ...   \n",
       "\n",
       "    f769   f771  f772  f773    f774    f775  nominal_3  nominal_4  f778  loss  \n",
       "0  -3.75   2.40 -1.73  1.34  0.2833 -0.2578          0          1    67     0  \n",
       "1  -4.61   2.88 -2.02  1.56  0.2747  0.2589          1          0    10     0  \n",
       "2  -2.59   1.83 -1.38  1.07  0.1956  0.4659          1          0    13     0  \n",
       "3  -4.87   2.51 -1.47  0.87  0.2109 -0.5439          1          0   393     0  \n",
       "4 -18.78  12.15 -8.72  6.56  0.2660  0.5302          0          1    10     0  \n",
       "\n",
       "[5 rows x 680 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3770d0-552f-4ff1-812e-01cfa4b5929e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5171c9c-7edd-4022-8121-d67e07c2f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('id', axis=1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428da398-b881-485b-bb39-90623392efa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5420, 1: 580})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "data.rename(columns = {'loan_status': 'target'},inplace = True)\n",
    "print(Counter(data.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4f8f45-37c6-4553-a8e4-b104a58661a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44bf3d47-b587-4acc-a4ca-ebe5353231b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = data.drop('target',axis = 1)\n",
    "df_target= data[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "056db65b-7613-4b0d-a57b-371d466bb478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5420, 1: 580})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(data.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c08d76-a86e-4bc0-b5a5-39ae88ae27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_df, df_target, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8266efc-0580-482b-a716-f5e8f31321f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3796, 1: 404})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "print(Counter(y_train['target']))\n",
    "sc = StandardScaler().set_output(transform=\"pandas\")\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a5dc7-8334-4616-a132-a8b97ea1d59d",
   "metadata": {},
   "source": [
    "#### smote-enn-enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ebbcfee-3181-45e0-8225-3c3982fd2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySMOTEENNENC:\n",
    "    \n",
    "    def __init__(self, categorical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "\n",
    "    def chk_neighbors(self, nn_object, additional_neighbor):\n",
    "        if isinstance(nn_object, Integral):\n",
    "            return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)\n",
    "        elif isinstance(nn_object, KNeighborsMixin):\n",
    "            return clone(nn_object)\n",
    "        else:\n",
    "            raise ValueError(\"nn_object should be either an integer or KNeighborsMixin instance\")\n",
    "    \n",
    "    def generate_samples(self, X, nn_data, nn_num, rows, cols, steps, continuous_features_):\n",
    "        rng = check_random_state(42)\n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            steps = sparse.csr_matrix(steps)\n",
    "            X_new = X[rows] + steps.multiply(diffs)\n",
    "        else:\n",
    "            X_new = X[rows] + steps * diffs\n",
    "\n",
    "        X_new = X_new.tolil() if sparse.issparse(X_new) else X_new\n",
    "        nn_data = nn_data.toarray() if sparse.issparse(nn_data) else nn_data\n",
    "\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        for idx in range(continuous_features_.size, X.shape[1]):\n",
    "            mode = stats.mode(all_neighbors[:, :, idx], axis=1)[0]\n",
    "            X_new[:, idx] = np.ravel(mode)\n",
    "        \n",
    "        return X_new\n",
    "    \n",
    "    def make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, continuous_features_, step_size=1.0):\n",
    "        random_state = check_random_state(42)\n",
    "        samples_indices = random_state.randint(low=0, high=len(nn_num.flatten()), size=n_samples)\n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self.generate_samples(X, nn_data, nn_num, rows, cols, steps, continuous_features_)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        \n",
    "        return X_new, y_new\n",
    "    \n",
    "    def cat_corr_pandas(self, X, target_df, target_column, target_value):\n",
    "        categorical_columns = list(X.columns)\n",
    "        X = pd.concat([X, target_df], axis=1)\n",
    "        is_target = X.loc[:, target_column] == target_value\n",
    "        X_filtered = X.loc[is_target, :]\n",
    "        X_filtered.drop(target_column, axis=1, inplace=True)\n",
    "        nrows = len(X)\n",
    "        encoded_dict_list = []\n",
    "        nan_dict = dict()\n",
    "        c = 0\n",
    "        imb_ratio = len(X_filtered) / len(X)\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            encoded_dict = {}\n",
    "            for level in list(X.loc[:, column].unique()):\n",
    "                row_level_filter = X.loc[:, column] == level\n",
    "                rows_in_level = len(X.loc[row_level_filter, :])\n",
    "                O = len(X.loc[is_target & row_level_filter, :])\n",
    "                E = rows_in_level * imb_ratio\n",
    "                \n",
    "                if E == 0:  # Prevent division by zero\n",
    "                    print(f\"Category '{level}' in column '{column}' has zero expected count (E = {E}). Skipping encoding.\")\n",
    "                    ENC = 0  # Default value if E == 0\n",
    "                else:\n",
    "                    ENC = (O - E) / E  # The encoding formula\n",
    "                \n",
    "                encoded_dict[level] = ENC\n",
    "\n",
    "            encoded_dict_list.append(encoded_dict)\n",
    "            X.loc[:, column] = X[column].map(encoded_dict)\n",
    "            nan_idx_array = np.ravel(np.argwhere(np.isnan(X.loc[:, column])))\n",
    "            \n",
    "            if len(nan_idx_array) > 0:\n",
    "                nan_dict[c] = nan_idx_array\n",
    "            c += 1\n",
    "            X.loc[:, column].fillna(-1, inplace=True)\n",
    "        \n",
    "        X.drop(target_column, axis=1, inplace=True)\n",
    "        return X, encoded_dict_list, nan_dict\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        X_cat_encoded, encoded_dict_list, nan_dict = self.cat_corr_pandas(X.iloc[:, np.asarray(self.categorical_features)], y, target_column='target', target_value=1)\n",
    "        X_cat_encoded = np.array(X_cat_encoded)\n",
    "        y = np.ravel(y)\n",
    "        X = np.array(X)\n",
    "\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        target_stats = dict(zip(unique, counts))\n",
    "        n_sample_majority = max(target_stats.values())\n",
    "        class_majority = max(target_stats, key=target_stats.get)\n",
    "        sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items() if key != class_majority}\n",
    "\n",
    "        n_features_ = X.shape[1]\n",
    "        categorical_features = np.asarray(self.categorical_features)\n",
    "        if categorical_features.dtype.name == 'bool':\n",
    "            categorical_features_ = np.flatnonzero(categorical_features)\n",
    "        else:\n",
    "            if any([cat not in np.arange(n_features_) for cat in categorical_features]):\n",
    "                raise ValueError('Some of the categorical indices are out of range. Indices should be between 0 and {}'.format(n_features_ - 1))\n",
    "            categorical_features_ = categorical_features\n",
    "\n",
    "        continuous_features_ = np.setdiff1d(np.arange(n_features_), categorical_features_)\n",
    "\n",
    "        target_stats = Counter(y)\n",
    "        class_minority = min(target_stats, key=target_stats.get)\n",
    "\n",
    "        X_continuous = X[:, continuous_features_]\n",
    "        X_continuous = check_array(X_continuous, accept_sparse=['csr', 'csc'])\n",
    "        X_minority = _safe_indexing(X_continuous, np.flatnonzero(y == class_minority))\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if X.format == 'csr':\n",
    "                _, var = sparse.csr_matrix(X_minority).mean(axis=0, keepdims=True), sparse.csr_matrix(X_minority).var(axis=0, keepdims=True)\n",
    "            else:\n",
    "                _, var = sparse.csc_matrix(X_minority).mean(axis=0, keepdims=True), sparse.csc_matrix(X_minority).var(axis=0, keepdims=True)\n",
    "        else:\n",
    "            var = X_minority.var(axis=0)\n",
    "        median_std_ = np.median(np.sqrt(var))\n",
    "\n",
    "        X_categorical = X[:, categorical_features_]\n",
    "        X_copy = np.hstack((X_continuous, X_categorical))\n",
    "        X_cat_encoded = X_cat_encoded * median_std_\n",
    "        X_encoded = np.hstack((X_continuous, X_cat_encoded))\n",
    "\n",
    "        X_resampled = X_encoded.copy()\n",
    "        y_resampled = y.copy()\n",
    "\n",
    "        for class_sample, n_samples in sampling_strategy.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "            nn_k_ = self.chk_neighbors(5, 1)\n",
    "            nn_k_.fit(X_class)\n",
    "\n",
    "            nns = nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self.make_samples(X_class, y.dtype, class_sample, X_class, nns, n_samples, continuous_features_, 1.0)\n",
    "\n",
    "            if sparse.issparse(X_new):\n",
    "                X_resampled = sparse.vstack([X_resampled, X_new])\n",
    "                sparse_func = 'tocsc' if X.format == 'csc' else 'tocsr'\n",
    "                X_resampled = getattr(X_resampled, sparse_func)()\n",
    "            else:\n",
    "                X_resampled = np.vstack((X_resampled, X_new))\n",
    "            y_resampled = np.hstack((y_resampled, y_new))\n",
    "\n",
    "        X_resampled_copy = X_resampled.copy()\n",
    "        i = 0\n",
    "        for col in range(continuous_features_.size, X.shape[1]):\n",
    "            encoded_dict = encoded_dict_list[i]\n",
    "            i += 1\n",
    "            for key, value in encoded_dict.items():\n",
    "                X_resampled_copy[:, col] = np.where(np.round(X_resampled_copy[:, col], 4) == np.round(value * median_std_, 4), key, X_resampled_copy[:, col])\n",
    "\n",
    "        for key, value in nan_dict.items():\n",
    "            for item in value:\n",
    "                X_resampled_copy[item, continuous_features_.size + key] = X_copy[item, continuous_features_.size + key]\n",
    "\n",
    "        X_resampled = X_resampled_copy\n",
    "        indices_reordered = np.argsort(np.hstack((continuous_features_, categorical_features_)))\n",
    "        if sparse.issparse(X_resampled):\n",
    "            col_indices = X_resampled.indices.copy()\n",
    "            for idx, col_idx in enumerate(indices_reordered):\n",
    "                mask = X_resampled.indices == col_idx\n",
    "                col_indices[mask] = idx\n",
    "            X_resampled.indices = col_indices\n",
    "        else:\n",
    "            X_resampled = X_resampled[:, indices_reordered]\n",
    "\n",
    "        return self.enn(X_resampled, y_resampled)\n",
    "    \n",
    "    def enn(self, X, y):\n",
    "        nn = KNeighborsClassifier(n_neighbors=3)\n",
    "        nn.fit(X, y)\n",
    "        y_pred = nn.predict(X)\n",
    "\n",
    "        mask = np.array([y[i] == y_pred[i] for i in range(len(y))])\n",
    "        X_clean = X[mask]\n",
    "        y_clean = y[mask]\n",
    "\n",
    "        return X_clean, y_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51f412-161d-4920-aa8c-27a3de70b341",
   "metadata": {},
   "source": [
    "##### geometric-smote-nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61f57b48-246b-4a15-913d-341e76df181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import check_random_state\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.utils.validation import check_array\n",
    "import numpy as np\n",
    "\n",
    "class GeometricSMOTENC:\n",
    "    \n",
    "    def __init__(self, categorical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "    def chk_neighbors(self, nn_object, additional_neighbor):\n",
    "        if isinstance(nn_object, int):\n",
    "            return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)\n",
    "        elif isinstance(nn_object, KNeighborsMixin):\n",
    "            return clone(nn_object)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported nn_object type: {type(nn_object)}\")\n",
    "\n",
    "    def generate_samples(self, X, nn_data, nn_num, rows, cols, steps, continuous_features_):\n",
    "        rng = check_random_state(42)\n",
    "\n",
    "        # Convert X and nn_data to NumPy arrays if they're DataFrames\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        if isinstance(nn_data, pd.DataFrame):\n",
    "            nn_data = nn_data.to_numpy()\n",
    "        \n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "        \n",
    "        # Geometric scaling of the difference\n",
    "        gamma = rng.uniform(low=0, high=1, size=steps.shape[0])\n",
    "        steps_geometric = np.power(diffs, gamma[:, np.newaxis])\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            sparse_func = type(X).__name__\n",
    "            steps_geometric = getattr(sparse, sparse_func)(steps_geometric)\n",
    "            X_new = X[rows] * steps_geometric\n",
    "        else:\n",
    "            X_new = X[rows] * steps_geometric\n",
    "\n",
    "        # Handle categorical features - use mode of nearest neighbors\n",
    "        nn_data = nn_data.toarray() if sparse.issparse(nn_data) else nn_data\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        # For categorical features, retain the most frequent category (mode) among the neighbors\n",
    "        for idx in continuous_features_:\n",
    "            mode = stats.mode(all_neighbors[:, :, idx], axis=1)[0]\n",
    "            X_new[:, idx] = np.ravel(mode)\n",
    "\n",
    "        return X_new\n",
    "    \n",
    "    def make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, continuous_features_, step_size=1.0):\n",
    "        random_state = check_random_state(42)\n",
    "        samples_indices = random_state.randint(low=0, high=len(nn_num.flatten()), size=n_samples)    \n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self.generate_samples(X, nn_data, nn_num, rows, cols, steps, continuous_features_)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        \n",
    "        return X_new, y_new\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        categorical_features = np.asarray(self.categorical_features)\n",
    "        n_features_ = X.shape[1]\n",
    "\n",
    "        if any([cat not in np.arange(n_features_) for cat in categorical_features]):\n",
    "            raise ValueError(f\"Some of the categorical indices are out of range. Indices should be between 0 and {n_features_}\")\n",
    "\n",
    "        categorical_features_ = np.flatnonzero(categorical_features)\n",
    "        continuous_features_ = np.setdiff1d(np.arange(n_features_), categorical_features_)\n",
    "\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        target_stats = dict(zip(unique, counts))\n",
    "        n_sample_majority = max(target_stats.values())\n",
    "        class_majority = max(target_stats, key=target_stats.get)\n",
    "        sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items() if key != class_majority}\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):  # Convert X to NumPy array\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        y = np.ravel(y)\n",
    "        minority_class = min(target_stats, key=target_stats.get)\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y.copy()\n",
    "\n",
    "        for class_sample, n_samples in sampling_strategy.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = X[target_class_indices]\n",
    "            nn_k_ = self.chk_neighbors(5, 1)\n",
    "            nn_k_.fit(X_class)\n",
    "\n",
    "            nns = nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self.make_samples(X, y.dtype, class_sample, X_class, nns, n_samples, continuous_features_, 1.0)\n",
    "\n",
    "            X_resampled = np.vstack((X_resampled, X_new))\n",
    "            y_resampled = np.hstack((y_resampled, y_new))\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865a10e-cdff-4d84-b316-0485f4ba4f11",
   "metadata": {},
   "source": [
    "### smote-enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7866f26c-736a-4541-b535-8bbb570efc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySMOTEENC:\n",
    "    def __init__(self, categorical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "\n",
    "    def chk_neighbors(self, nn_object, additional_neighbor):\n",
    "        if isinstance(nn_object, Integral):\n",
    "            return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)\n",
    "        elif isinstance(nn_object, KNeighborsMixin):\n",
    "            return clone(nn_object)\n",
    "        else:\n",
    "            raise TypeError(\"Invalid neighbor object type.\")     \n",
    "\n",
    "    def generate_samples(self, X, nn_data, nn_num, rows, cols, steps, continuous_features_):\n",
    "        rng = check_random_state(42)\n",
    "\n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            sparse_func = type(X).__name__\n",
    "            steps = getattr(sparse, sparse_func)(steps)\n",
    "            X_new = X[rows] + steps.multiply(diffs)\n",
    "        else:\n",
    "            X_new = X[rows] + steps * diffs \n",
    "\n",
    "        X_new = (X_new.tolil() if sparse.issparse(X_new) else X_new)\n",
    "        # convert to dense array since scipy.sparse doesn't handle 3D\n",
    "        nn_data = (nn_data.toarray() if sparse.issparse(nn_data) else nn_data)\n",
    "\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        for idx in range(continuous_features_.size, X.shape[1]):\n",
    "            mode = stats.mode(all_neighbors[:, :, idx], axis=1)[0]\n",
    "            X_new[:, idx] = np.ravel(mode)\n",
    "            \n",
    "        return X_new\n",
    "\n",
    "    def make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, continuous_features_, step_size=1.0):\n",
    "        random_state = check_random_state(42)\n",
    "        samples_indices = random_state.randint(low=0, high=len(nn_num.flatten()), size=n_samples)    \n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self.generate_samples(X, nn_data, nn_num, rows, cols, steps, continuous_features_)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        \n",
    "        return X_new, y_new\n",
    "\n",
    "    def cat_corr_pandas(self, X, target_df, target_column, target_value):\n",
    "        # X has categorical columns\n",
    "        categorical_columns = list(X.columns)\n",
    "        X = pd.concat([X, target_df], axis=1)\n",
    "\n",
    "        # filter X for target value\n",
    "        is_target = X.loc[:, target_column] == target_value\n",
    "        X_filtered = X.loc[is_target, :]\n",
    "\n",
    "        X_filtered.drop(target_column, axis=1, inplace=True)\n",
    "\n",
    "        # get columns in X\n",
    "        nrows = len(X)\n",
    "        encoded_dict_list = []\n",
    "        nan_dict = dict({})\n",
    "        c = 0\n",
    "        imb_ratio = len(X_filtered) / len(X) if len(X) > 0 else 0 \n",
    "        OE_dict = {}\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            for level in list(X.loc[:, column].unique()):\n",
    "                \n",
    "                # filter rows where level is present\n",
    "                row_level_filter = X.loc[:, column] == level\n",
    "                rows_in_level = len(X.loc[row_level_filter, :])\n",
    "                \n",
    "                # number of rows in level where target is 1\n",
    "                O = len(X.loc[is_target & row_level_filter, :])\n",
    "                E = rows_in_level * imb_ratio\n",
    "                # Encoded value = chi, i.e. (observed - expected)/expected\n",
    "                if E == 0:\n",
    "                    ENC = 0\n",
    "                else:\n",
    "                    ENC = (O - E) / E\n",
    "\n",
    "                OE_dict[level] = ENC\n",
    "                \n",
    "            encoded_dict_list.append(OE_dict)\n",
    "\n",
    "            X.loc[:, column] = X[column].map(OE_dict)\n",
    "            nan_idx_array = np.ravel(np.argwhere(np.isnan(X.loc[:, column])))\n",
    "            \n",
    "            if len(nan_idx_array) > 0:\n",
    "                nan_dict[c] = nan_idx_array\n",
    "            c = c + 1\n",
    "            X.loc[:, column].fillna(-1, inplace=True)\n",
    "            \n",
    "        X.drop(target_column, axis=1, inplace=True)\n",
    "        return X, encoded_dict_list, nan_dict\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        X_cat_encoded, encoded_dict_list, nan_dict = self.cat_corr_pandas(X.iloc[:, np.asarray(self.categorical_features)], y, target_column='target', target_value=1)\n",
    "        X_cat_encoded = np.array(X_cat_encoded)\n",
    "        y = np.ravel(y)\n",
    "        X = np.array(X)\n",
    "\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        target_stats = dict(zip(unique, counts))\n",
    "        n_sample_majority = max(target_stats.values())\n",
    "        class_majority = max(target_stats, key=target_stats.get)\n",
    "        sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items() if key != class_majority}\n",
    "\n",
    "        n_features_ = X.shape[1]\n",
    "        categorical_features = np.asarray(self.categorical_features)\n",
    "        if categorical_features.dtype.name == 'bool':\n",
    "            categorical_features_ = np.flatnonzero(categorical_features)\n",
    "        else:\n",
    "            if any([cat not in np.arange(n_features_) for cat in categorical_features]):\n",
    "                raise ValueError('Some of the categorical indices are out of range. Indices'\n",
    "                            ' should be between 0 and {}'.format(n_features_))\n",
    "            categorical_features_ = categorical_features\n",
    "\n",
    "        continuous_features_ = np.setdiff1d(np.arange(n_features_), categorical_features_)\n",
    "\n",
    "        target_stats = Counter(y)\n",
    "        class_minority = min(target_stats, key=target_stats.get)\n",
    "\n",
    "        X_continuous = X[:, continuous_features_]\n",
    "        X_continuous = check_array(X_continuous, accept_sparse=['csr', 'csc'])\n",
    "        X_minority = _safe_indexing(X_continuous, np.flatnonzero(y == class_minority))\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if X.format == 'csr':\n",
    "                _, var = sparsefuncs_fast.csr_mean_variance_axis0(X_minority)\n",
    "            else:\n",
    "                _, var = sparsefuncs_fast.csc_mean_variance_axis0(X_minority)\n",
    "        else:\n",
    "            var = X_minority.var(axis=0)\n",
    "        median_std_ = np.median(np.sqrt(var))\n",
    "\n",
    "        X_categorical = X[:, categorical_features_]\n",
    "        X_copy = np.hstack((X_continuous, X_categorical))\n",
    "\n",
    "        X_cat_encoded = X_cat_encoded * median_std_\n",
    "\n",
    "        X_encoded = np.hstack((X_continuous, X_cat_encoded))\n",
    "        X_resampled = X_encoded.copy()\n",
    "        y_resampled = y.copy()\n",
    "\n",
    "        for class_sample, n_samples in sampling_strategy.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "            nn_k_ = self.chk_neighbors(5, 1)\n",
    "            nn_k_.fit(X_class)\n",
    "\n",
    "            nns = nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self.make_samples(X_class, y.dtype, class_sample, X_class, nns, n_samples, continuous_features_, 1.0)\n",
    "\n",
    "            if sparse.issparse(X_new):\n",
    "                X_resampled = sparse.vstack([X_resampled, X_new])\n",
    "                sparse_func = 'tocsc' if X.format == 'csc' else 'tocsr'\n",
    "                X_resampled = getattr(X_resampled, sparse_func)()\n",
    "            else:\n",
    "                X_resampled = np.vstack((X_resampled, X_new))\n",
    "            y_resampled = np.hstack((y_resampled, y_new))\n",
    "            \n",
    "        X_resampled_copy = X_resampled.copy()\n",
    "        i = 0\n",
    "        for col in range(continuous_features_.size, X.shape[1]):\n",
    "            encoded_dict = encoded_dict_list[i]\n",
    "            i = i + 1\n",
    "            for key, value in encoded_dict.items():\n",
    "                X_resampled_copy[:, col] = np.where(np.round(X_resampled_copy[:, col], 4) == np.round(value * median_std_, 4), key, X_resampled_copy[:, col])\n",
    "\n",
    "        for key, value in nan_dict.items():\n",
    "            for item in value:\n",
    "                X_resampled_copy[item, continuous_features_.size + key] = X_copy[item, continuous_features_.size + key]\n",
    "\n",
    "               \n",
    "        X_resampled = X_resampled_copy   \n",
    "        indices_reordered = np.argsort(np.hstack((continuous_features_, categorical_features_)))\n",
    "        if sparse.issparse(X_resampled):\n",
    "            col_indices = X_resampled.indices.copy()\n",
    "            for idx, col_idx in enumerate(indices_reordered):\n",
    "                mask = X_resampled.indices == col_idx\n",
    "                col_indices[mask] = idx\n",
    "            X_resampled.indices = col_indices\n",
    "        else:\n",
    "            X_resampled = X_resampled[:, indices_reordered]\n",
    "        return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1468663-943c-438e-8bff-017245d54e91",
   "metadata": {},
   "source": [
    "### smote-nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61dab9bc-f624-45b7-97f7-d68c6cd2ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "from sklearn.utils import check_random_state\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class SMOTENC:\n",
    "    def __init__(self, categorical_features, k_neighbors=5, random_state=42, n_jobs=-1):\n",
    "        self.categorical_features = np.asarray(categorical_features)\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _generate_synthetic_sample(self, X_cont, X_cat, neighbors, rng, idx):\n",
    "        \"\"\"Generate a single synthetic sample\"\"\"\n",
    "        neighbor_idx = rng.choice(neighbors[idx])\n",
    "        alpha = rng.uniform()\n",
    "        \n",
    "        # Interpolate continuous features\n",
    "        cont_new = X_cont[idx] + alpha * (X_cont[neighbor_idx] - X_cont[idx])\n",
    "        \n",
    "        # Randomly select categorical features\n",
    "        cat_new = np.where(rng.random(len(X_cat[idx])) < 0.5,\n",
    "                         X_cat[idx], \n",
    "                         X_cat[neighbor_idx])\n",
    "        \n",
    "        return np.hstack((cont_new, cat_new))\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        \"\"\"Optimized implementation of SMOTE-NC resampling\"\"\"\n",
    "        rng = check_random_state(self.random_state)\n",
    "        \n",
    "        # Convert to numpy arrays if DataFrames\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "            y = y.values\n",
    "        \n",
    "        # Convert y to 1D array of hashable types (strings or numbers)\n",
    "        y = np.asarray(y).flatten()\n",
    "        \n",
    "        # Identify minority and majority classes\n",
    "        try:\n",
    "            class_counts = Counter(y)\n",
    "        except TypeError:\n",
    "            # If elements are unhashable (like arrays), convert to tuples\n",
    "            y = [tuple(row) if isinstance(row, np.ndarray) else row for row in y]\n",
    "            class_counts = Counter(y)\n",
    "            \n",
    "        if len(class_counts) < 2:\n",
    "            return X, y\n",
    "            \n",
    "        minority_class = min(class_counts, key=class_counts.get)\n",
    "        majority_class = max(class_counts, key=class_counts.get)\n",
    "        \n",
    "        # Calculate number of synthetic samples needed\n",
    "        n_minority = class_counts[minority_class]\n",
    "        n_majority = class_counts[majority_class]\n",
    "        n_synthetic = n_majority - n_minority\n",
    "        \n",
    "        if n_synthetic <= 0:\n",
    "            return X, y\n",
    "\n",
    "        # Get minority samples\n",
    "        minority_mask = (y == minority_class) if not isinstance(minority_class, tuple) else \\\n",
    "                       [tuple(row) == minority_class if isinstance(row, np.ndarray) else row == minority_class for row in y]\n",
    "        X_minority = X[minority_mask]\n",
    "        \n",
    "        # Split continuous and categorical features\n",
    "        continuous_features = np.setdiff1d(np.arange(X.shape[1]), self.categorical_features)\n",
    "        X_cont = X_minority[:, continuous_features]\n",
    "        X_cat = X_minority[:, self.categorical_features]\n",
    "        \n",
    "        # Find nearest neighbors\n",
    "        nn = NearestNeighbors(n_neighbors=self.k_neighbors+1, n_jobs=self.n_jobs)\n",
    "        nn.fit(X_cont)\n",
    "        neighbors = nn.kneighbors(X_cont, return_distance=False)[:, 1:]  # Exclude self\n",
    "        \n",
    "        # Generate synthetic samples in parallel\n",
    "        synthetic_samples = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._generate_synthetic_sample)(\n",
    "                X_cont, X_cat, neighbors, rng, \n",
    "                rng.randint(0, len(X_minority))\n",
    "            ) for _ in range(n_synthetic)\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        synthetic_samples = np.array(synthetic_samples)\n",
    "        \n",
    "        # Reorder features to original order\n",
    "        feature_order = np.argsort(np.hstack((continuous_features, self.categorical_features)))\n",
    "        synthetic_samples = synthetic_samples[:, feature_order]\n",
    "        \n",
    "        X_resampled = np.vstack((X, synthetic_samples))\n",
    "        y_resampled = np.hstack((y, np.full(n_synthetic, minority_class)))\n",
    "        \n",
    "        return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d989249-b006-4f00-b5da-48864f4431e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [0,3,4,5,6,8,9,10,11,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd8ee3bb-c97c-45ea-ae6e-07a9a73a7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SMOTE NC \n",
    "\n",
    "smote_nc = SMOTENC(categorical_features=categorical_features, \n",
    "                   k_neighbors=3, \n",
    "                   random_state=42,\n",
    "                   n_jobs=-1)  # Use all available cores\n",
    "\n",
    "# Apply SMOTENC\n",
    "X_S_NC, y_S_NC = smote_nc.fit_resample(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe7aca9-50d8-4330-912c-0c859ebfc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LETS APPLY SMOTE_enn-ENC TO OUR DATA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "samp_pipeline = MySMOTEENNENC(categorical_features = categorical_features)\n",
    "X_ENN_ENC, y_ENN_ENC = samp_pipeline.fit_resample(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "155e8e0c-6572-4165-89cb-f21c53bdf0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = list(X_train_sc.columns)\n",
    "X_ENN_ENC = pd.DataFrame(X_ENN_ENC, index=range(X_ENN_ENC.shape[0]),\n",
    "                          columns=col_list)\n",
    "y_ENN_ENC= pd.DataFrame(y_ENN_ENC)\n",
    "y_ENN_ENC['target'] = y_ENN_ENC[0]\n",
    "y_ENN_ENC.drop(0,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81fc2b39-bcfc-40cf-b32b-f26c2dfa50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run the Geometric SMOTENC\n",
    "g_smote = GeometricSMOTENC(categorical_features=categorical_features)\n",
    "X_g_smote, y_g_smote = g_smote.fit_resample(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba4c3905-26d0-48a5-b754-064d291af9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LETS APPLY SMOTE_ENC TO OUR DATA\n",
    "samp_pipeline = MySMOTEENC(categorical_features = categorical_features )\n",
    "X_ENC, y_ENC = samp_pipeline.fit_resample(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b15f16-b559-4257-948f-4f95c6a3b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = list(X_train_sc.columns)\n",
    "X_ENC = pd.DataFrame(X_ENC, index=range(X_ENC.shape[0]),\n",
    "                          columns=col_list)\n",
    "y_ENC= pd.DataFrame(y_ENC)\n",
    "y_ENC['target'] = y_ENC[0]\n",
    "y_ENC.drop(0,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e642a8f-d9a9-477b-b3fc-dc2ab0b0d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3d26379-df6c-42bd-b981-11984306c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Adaptive Lasso with Random Forest: 100%|█| 100/100 [3:50:38<00:00, 138."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iteration  balanced_accuracy  precision    recall  f1_score   roc_auc  \\\n",
      "0          1           0.856597   0.658416  0.755682  0.703704  0.940346   \n",
      "1          2           0.818406   0.245614  0.954545  0.390698  0.950340   \n",
      "2          3           0.915053   0.806452  0.852273  0.828729  0.972287   \n",
      "3          4           0.891961   0.574713  0.852273  0.686499  0.955787   \n",
      "4          5           0.844436   0.945736  0.693182  0.800000  0.976438   \n",
      "\n",
      "     g_mean    pr_auc  threshold  \n",
      "0  0.850632  0.805229   0.474606  \n",
      "1  0.807003  0.843270   0.418363  \n",
      "2  0.912896  0.913388   0.441372  \n",
      "3  0.891078  0.869187   0.462660  \n",
      "4  0.830779  0.897761   0.489590  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define Adaptive Lasso functions\n",
    "alpha = 0.000011\n",
    "gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize the imputer to fill missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer to the training and test data\n",
    "X_ENN_ENC_imputed = imputer.fit_transform(X_ENN_ENC)\n",
    "X_test_sc_imputed = imputer.transform(X_test_sc)  # Make sure this is defined\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "# Train and evaluate Adaptive Lasso 100 times using Random Forest\n",
    "for i in tqdm(range(1, 101), desc=\"Training Adaptive Lasso with Random Forest\"):\n",
    "    # Randomly sample 80% of the data (introduces variation)\n",
    "    X_train_subset, y_train_subset = resample(\n",
    "        X_ENN_ENC_imputed, y_ENN_ENC, n_samples=int(0.8 * len(X_ENN_ENC_imputed)), random_state=i\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_subset, y_train_subset, random_state=i)\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    X_train_array = np.asarray(X_train_shuffled)\n",
    "\n",
    "    # Initialize weights randomly (0.1 to 10.0)\n",
    "    np.random.seed(i)\n",
    "    weights = np.random.uniform(0.1, 10.0, size=X_train_array.shape[1])\n",
    "\n",
    "    # Randomize alpha slightly\n",
    "    current_alpha = alpha * np.random.uniform(0.8, 1.2)\n",
    "\n",
    "    # Iterative process for Adaptive Lasso\n",
    "    for _ in range(5):\n",
    "        X_w = X_train_array / weights\n",
    "        X_w = X_w + np.random.normal(0, 1e-2, X_w.shape)  # Larger jitter\n",
    "\n",
    "        # Fit Random Forest model\n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=250,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=i,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_w, y_train_shuffled)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Update weights based on feature importances\n",
    "        new_weights = np.asarray(gprime(model.feature_importances_))\n",
    "        if new_weights.shape[0] != X_train_array.shape[1]:\n",
    "            new_weights = np.pad(new_weights, (0, X_train_array.shape[1] - new_weights.shape[0]), 'constant', constant_values=1)\n",
    "        weights = new_weights.reshape(-1)\n",
    "\n",
    "    # Predict with random threshold (0.4 to 0.6)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test_sc_imputed)[:, 1]\n",
    "        threshold = np.random.uniform(0.4, 0.6)\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        g_mean = np.sqrt(recall * specificity)\n",
    "\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba)\n",
    "        pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "        # Store results\n",
    "        results.append([\n",
    "            i, balanced_accuracy, precision, recall, f1, roc_auc, g_mean, pr_auc, threshold\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"Error in iteration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['iteration', 'balanced_accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'g_mean', 'pr_auc', 'threshold']\n",
    ")\n",
    "\n",
    "# Save results\n",
    "df_results.to_csv('LOANDEFAULT_RF_ALASSO_RF(SMOTE_ENNENC).csv', index=False)\n",
    "\n",
    "# Display\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66a09491-dfea-452d-8001-4b0649bc4e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Adaptive Lasso with Random Forest: 100%|█| 100/100 [4:34:21<00:00, 164."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iteration  balanced_accuracy  precision    recall  f1_score   roc_auc  \\\n",
      "0          1           0.994318   1.000000  0.988636  0.994286  1.000000   \n",
      "1          2           0.919643   0.402746  1.000000  0.574225  0.999508   \n",
      "2          3           0.871921   0.297297  1.000000  0.458333  0.999904   \n",
      "3          4           0.968750   1.000000  0.937500  0.967742  0.999993   \n",
      "4          5           0.994010   0.994286  0.988636  0.991453  0.999979   \n",
      "\n",
      "     g_mean    pr_auc  threshold  \n",
      "0  0.994302  1.000000   0.532542  \n",
      "1  0.916125  0.997115   0.435739  \n",
      "2  0.862463  0.999232   0.418162  \n",
      "3  0.968246  0.999935   0.546331  \n",
      "4  0.993996  0.999809   0.502438  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define Adaptive Lasso functions\n",
    "alpha = 0.000011\n",
    "gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize the imputer to fill missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer to the training and test data\n",
    "X_ENC_imputed = imputer.fit_transform(X_ENC)\n",
    "X_test_sc_imputed = imputer.transform(X_test_sc)  # Make sure this is defined\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "# Train and evaluate Adaptive Lasso 100 times using Random Forest\n",
    "for i in tqdm(range(1, 101), desc=\"Training Adaptive Lasso with Random Forest\"):\n",
    "    # Randomly sample 80% of the data (introduces variation)\n",
    "    X_train_subset, y_train_subset = resample(\n",
    "        X_ENC_imputed, y_ENC, n_samples=int(0.8 * len(X_ENC_imputed)), random_state=i\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_subset, y_train_subset, random_state=i)\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    X_train_array = np.asarray(X_train_shuffled)\n",
    "\n",
    "    # Initialize weights randomly (0.1 to 10.0)\n",
    "    np.random.seed(i)\n",
    "    weights = np.random.uniform(0.1, 10.0, size=X_train_array.shape[1])\n",
    "\n",
    "    # Randomize alpha slightly\n",
    "    current_alpha = alpha * np.random.uniform(0.8, 1.2)\n",
    "\n",
    "    # Iterative process for Adaptive Lasso\n",
    "    for _ in range(5):\n",
    "        X_w = X_train_array / weights\n",
    "        X_w = X_w + np.random.normal(0, 1e-2, X_w.shape)  # Larger jitter\n",
    "\n",
    "        # Fit model with variable regularization\n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=250,\n",
    "                random_state=i,\n",
    "                class_weight='balanced'  # Added to handle class imbalance\n",
    "            )\n",
    "            model.fit(X_w, y_train_shuffled)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Update weights using feature importances instead of coefficients\n",
    "        new_weights = np.asarray(gprime(model.feature_importances_))\n",
    "        if new_weights.shape[0] != X_train_array.shape[1]:\n",
    "            new_weights = np.pad(new_weights, (0, X_train_array.shape[1] - new_weights.shape[0]), 'constant', constant_values=1)\n",
    "        weights = new_weights.reshape(-1)\n",
    "\n",
    "    # Predict with random threshold (0.4 to 0.6)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test_sc_imputed)[:, 1]\n",
    "        threshold = np.random.uniform(0.4, 0.6)\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        g_mean = np.sqrt(recall * specificity)\n",
    "\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba)\n",
    "        pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "        # Store results\n",
    "        results.append([\n",
    "            i, balanced_accuracy, precision, recall, f1, roc_auc, g_mean, pr_auc, threshold\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"Error in iteration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['iteration', 'balanced_accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'g_mean', 'pr_auc', 'threshold']\n",
    ")\n",
    "\n",
    "# Save results\n",
    "df_results.to_csv('LOANDEFAULT_RF_ALASSO(SMOTE_ENC).csv', index=False)\n",
    "\n",
    "# Display\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "052a0da0-0288-4a2a-98d3-b84187056483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Adaptive Lasso with Random Forest: 100%|█| 100/100 [4:59:53<00:00, 179."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iteration  balanced_accuracy  precision    recall  f1_score   roc_auc  \\\n",
      "0          1           0.724432        1.0  0.448864  0.619608  1.000000   \n",
      "1          2           0.974432        1.0  0.948864  0.973761  0.999986   \n",
      "2          3           0.968750        1.0  0.937500  0.967742  1.000000   \n",
      "3          4           0.707386        1.0  0.414773  0.586345  0.999972   \n",
      "4          5           0.826705        1.0  0.653409  0.790378  1.000000   \n",
      "\n",
      "   threshold  specificity    g_mean    pr_auc  \n",
      "0   0.588066          1.0  0.669973  1.000000  \n",
      "1   0.465870          1.0  0.974096  0.999873  \n",
      "2   0.471719          1.0  0.968246  1.000000  \n",
      "3   0.557373          1.0  0.644029  0.999753  \n",
      "4   0.521814          1.0  0.808337  1.000000  \n",
      "\n",
      "Top 10 most important features:\n",
      "     feature  importance\n",
      "451      451   36.749497\n",
      "169      169   34.865670\n",
      "626      626   34.188342\n",
      "588      588   33.407436\n",
      "167      167   32.867982\n",
      "590      590   32.654660\n",
      "392      392   32.097142\n",
      "114      114   32.088290\n",
      "222      222   31.523712\n",
      "30        30   31.361052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define Adaptive Lasso functions\n",
    "alpha = 0.000011\n",
    "gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize the imputer to fill missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer to the training and test data\n",
    "X_S_NC_imputed = imputer.fit_transform(X_S_NC)\n",
    "X_test_sc_imputed = imputer.transform(X_test_sc)\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "# Train and evaluate Adaptive Lasso 100 times using Random Forest\n",
    "for i in tqdm(range(1, 101), desc=\"Training Adaptive Lasso with Random Forest\"):\n",
    "    # Randomly sample 80% of the data\n",
    "    X_train_subset, y_train_subset = resample(\n",
    "        X_S_NC_imputed, y_S_NC, n_samples=int(0.8 * len(X_S_NC_imputed)), random_state=i\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_subset, y_train_subset, random_state=i)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = np.ones(X_train_shuffled.shape[1])\n",
    "    \n",
    "    # Iterative process for Adaptive Lasso\n",
    "    for _ in range(5):\n",
    "        # Apply weights to features\n",
    "        X_weighted = X_train_shuffled / weights\n",
    "        \n",
    "        # Add small noise for stability\n",
    "        X_weighted = X_weighted + np.random.normal(0, 1e-3, X_weighted.shape)\n",
    "        \n",
    "        # Fit Random Forest\n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=250,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=i,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_weighted, y_train_shuffled)\n",
    "            \n",
    "            # Update weights using feature importances\n",
    "            weights = gprime(model.feature_importances_)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Make predictions\n",
    "    try:\n",
    "        # Apply final weights to test data\n",
    "        X_test_weighted = X_test_sc_imputed / weights\n",
    "        \n",
    "        y_proba = model.predict_proba(X_test_weighted)[:, 1]\n",
    "        threshold = np.random.uniform(0.4, 0.6)\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'iteration': i,\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'threshold': threshold\n",
    "        }\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        metrics.update({\n",
    "            'specificity': tn / (tn + fp),\n",
    "            'g_mean': np.sqrt(metrics['recall'] * (tn / (tn + fp))),\n",
    "            'pr_auc': auc(*precision_recall_curve(y_test, y_proba)[1::-1])\n",
    "        })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error in iteration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save and display results\n",
    "df_results.to_csv('LOANDEFAULT_RF_ADAPTIVE_LASSO(SMOTE_NC).csv', index=False)\n",
    "print(df_results.head())\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': range(X_S_NC_imputed.shape[1]),\n",
    "    'importance': weights\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de5a12-b307-476c-9ba5-490886044956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Adaptive Lasso with Random Forest:  47%|▍| 47/100 [2:25:20<2:40:37, 181"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define Adaptive Lasso functions\n",
    "alpha = 0.000011\n",
    "gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize the imputer to fill missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer to the training and test data\n",
    "X_g_smote_imputed = imputer.fit_transform(X_g_smote)\n",
    "X_test_sc_imputed = imputer.transform(X_test_sc)\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "# Train and evaluate Adaptive Lasso 100 times using Random Forest\n",
    "for i in tqdm(range(1, 101), desc=\"Training Adaptive Lasso with Random Forest\"):\n",
    "    # Randomly sample 80% of the data (introduces variation)\n",
    "    X_train_subset, y_train_subset = resample(\n",
    "        X_g_smote_imputed, y_g_smote, \n",
    "        n_samples=int(0.8 * len(X_g_smote_imputed)), \n",
    "        random_state=i\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_subset, y_train_subset, random_state=i)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = np.ones(X_train_shuffled.shape[1])\n",
    "    \n",
    "    # Iterative process for Adaptive Lasso\n",
    "    for _ in range(5):\n",
    "        # Apply weights to features\n",
    "        X_weighted = X_train_shuffled / weights\n",
    "        \n",
    "        # Add small noise for stability\n",
    "        X_weighted = X_weighted + np.random.normal(0, 1e-3, X_weighted.shape)\n",
    "        \n",
    "        # Fit Random Forest\n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=250,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=i,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_weighted, y_train_shuffled)\n",
    "            \n",
    "            # Update weights using feature importances\n",
    "            weights = gprime(model.feature_importances_)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Make predictions\n",
    "    try:\n",
    "        # Apply final weights to test data\n",
    "        X_test_weighted = X_test_sc_imputed / weights\n",
    "        \n",
    "        y_proba = model.predict_proba(X_test_weighted)[:, 1]\n",
    "        threshold = np.random.uniform(0.4, 0.6)\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'iteration': i,\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'threshold': threshold\n",
    "        }\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        metrics.update({\n",
    "            'specificity': tn / (tn + fp),\n",
    "            'g_mean': np.sqrt(metrics['recall'] * (tn / (tn + fp))),\n",
    "            'pr_auc': auc(*precision_recall_curve(y_test, y_proba)[1::-1])\n",
    "        })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error in iteration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save and display results\n",
    "df_results.to_csv('LOANDEFAULT_RF_ADAPTIVE_LASSO(GSMOTE_NC).csv', index=False)\n",
    "print(df_results.head())\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': range(X_g_smote_imputed.shape[1]),\n",
    "    'importance': weights\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba62b0-cfc5-4d60-8677-90c323865391",
   "metadata": {},
   "source": [
    "#### ADAPTIVE ELASTIC NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76e40f-b7b7-4155-b9c7-6e34ce9aafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define Adaptive Elastic Net parameters with randomization\n",
    "base_alpha = 0.1  # Base regularization strength\n",
    "gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer to data\n",
    "X_ENN_ENC_imputed = imputer.fit_transform(X_ENN_ENC)\n",
    "X_test_sc_imputed = imputer.transform(X_test_sc)\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(1, 101), desc=\"Training Adaptive ElasticNet with RF\"):\n",
    "    # Random subsampling (80% of data)\n",
    "    X_train_subset, y_train_subset = resample(\n",
    "        X_ENN_ENC_imputed, y_ENN_ENC, \n",
    "        n_samples=int(0.8 * len(X_ENN_ENC_imputed)), \n",
    "        random_state=i\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_subset, y_train_subset, random_state=i)\n",
    "    X_train_array = np.asarray(X_train_shuffled)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = np.ones(X_train_array.shape[1])\n",
    "    \n",
    "    # Randomized hyperparameters\n",
    "    current_alpha = base_alpha * np.random.uniform(0.7, 1.3)\n",
    "\n",
    "    # Adaptive Elastic Net iterations\n",
    "    for _ in range(5):\n",
    "        X_w = X_train_array / weights\n",
    "        X_w = X_w + np.random.normal(0, 1e-3, X_w.shape)  # Small jitter\n",
    "        \n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=250,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=i,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_w, y_train_shuffled)\n",
    "            \n",
    "            # Update weights using feature importances\n",
    "            weights = gprime(model.feature_importances_)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Prediction with random threshold\n",
    "    try:\n",
    "        # Apply final weights to test data\n",
    "        X_test_weighted = X_test_sc_imputed / weights\n",
    "        \n",
    "        y_proba = model.predict_proba(X_test_weighted)[:, 1]\n",
    "        threshold = np.random.uniform(0.4, 0.6)\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'iteration': i,\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'threshold': threshold,\n",
    "            'alpha': current_alpha\n",
    "        }\n",
    "        \n",
    "        # Additional metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        metrics.update({\n",
    "            'specificity': tn / (tn + fp),\n",
    "            'g_mean': np.sqrt(metrics['recall'] * (tn / (tn + fp))),\n",
    "            'pr_auc': auc(*precision_recall_curve(y_test, y_proba)[1::-1])\n",
    "        })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error in iteration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "df_results.to_csv('LOANDEFAULT_RF_Adaptive_ElasticNet(SMOTE_ENN_ENC).csv', index=False)\n",
    "\n",
    "# Display results and feature importances\n",
    "print(df_results.head())\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': range(X_ENN_ENC_imputed.shape[1]),\n",
    "    'importance': weights\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e2617-7198-4623-a7ac-07ee9431dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define adaptive weighting function\n",
    "gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer to data\n",
    "X_ENC_imputed = imputer.fit_transform(X_ENC)\n",
    "X_test_sc_imputed = imputer.transform(X_test_sc)\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(1, 101), desc=\"Training Adaptive Random Forest\"):\n",
    "    # Random subsampling (80% of data)\n",
    "    X_train_subset, y_train_subset = resample(\n",
    "        X_ENC_imputed, y_ENC, \n",
    "        n_samples=int(0.8 * len(X_ENC_imputed)), \n",
    "        random_state=i\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_subset, y_train_subset, random_state=i)\n",
    "    X_train_array = np.asarray(X_train_shuffled)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = np.ones(X_train_array.shape[1])\n",
    "    \n",
    "    # Adaptive weighting iterations\n",
    "    for _ in range(5):\n",
    "        # Apply weights to features\n",
    "        X_weighted = X_train_array / weights\n",
    "        X_weighted = X_weighted + np.random.normal(0, 1e-3, X_weighted.shape)  # Small jitter\n",
    "        \n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=250,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=i,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_weighted, y_train_shuffled)\n",
    "            \n",
    "            # Update weights using feature importances\n",
    "            weights = gprime(model.feature_importances_)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training error in iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Make predictions\n",
    "    try:\n",
    "        # Apply final weights to test data\n",
    "        X_test_weighted = X_test_sc_imputed / weights\n",
    "        \n",
    "        y_proba = model.predict_proba(X_test_weighted)[:, 1]\n",
    "        threshold = np.random.uniform(0.4, 0.6)\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'iteration': i,\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'threshold': threshold\n",
    "        }\n",
    "        \n",
    "        # Additional metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        metrics.update({\n",
    "            'specificity': tn / (tn + fp),\n",
    "            'g_mean': np.sqrt(metrics['recall'] * (tn / (tn + fp))),\n",
    "            'pr_auc': auc(*precision_recall_curve(y_test, y_proba)[1::-1])\n",
    "        })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error in iteration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "df_results.to_csv('LOANDEFAULT_RF_Adaptive_Weighting(SMOTE_ENC).csv', index=False)\n",
    "\n",
    "# Display results and feature importances\n",
    "print(df_results.head())\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': range(X_ENC_imputed.shape[1]),\n",
    "    'importance': weights\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824025e-14e8-4032-ba83-c74e97b1dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define adaptive weighting function\n",
    "gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer to data\n",
    "X_g_smote_imputed = imputer.fit_transform(X_g_smote)\n",
    "X_test_sc_imputed = imputer.transform(X_test_sc)\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(1, 101), desc=\"Training Adaptive Random Forest\"):\n",
    "    # Random subsampling (80% of data)\n",
    "    X_train_subset, y_train_subset = resample(\n",
    "        X_g_smote_imputed, y_g_smote,\n",
    "        n_samples=int(0.8 * len(X_g_smote_imputed)),\n",
    "        random_state=i\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_subset, y_train_subset, random_state=i)\n",
    "    X_train_array = np.asarray(X_train_shuffled)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = np.ones(X_train_array.shape[1])\n",
    "    \n",
    "    # Adaptive weighting iterations\n",
    "    for _ in range(5):\n",
    "        # Apply weights to features\n",
    "        X_weighted = X_train_array / weights\n",
    "        X_weighted = X_weighted + np.random.normal(0, 1e-3, X_weighted.shape)  # Small jitter\n",
    "        \n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=250,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=i,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_weighted, y_train_shuffled)\n",
    "            \n",
    "            # Update weights using feature importances\n",
    "            weights = gprime(model.feature_importances_)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training error in iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Make predictions\n",
    "    try:\n",
    "        # Apply final weights to test data\n",
    "        X_test_weighted = X_test_sc_imputed / weights\n",
    "        \n",
    "        y_proba = model.predict_proba(X_test_weighted)[:, 1]\n",
    "        threshold = np.random.uniform(0.4, 0.6)\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'iteration': i,\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'threshold': threshold\n",
    "        }\n",
    "        \n",
    "        # Additional metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        metrics.update({\n",
    "            'specificity': tn / (tn + fp),\n",
    "            'g_mean': np.sqrt(metrics['recall'] * (tn / (tn + fp))),\n",
    "            'pr_auc': auc(*precision_recall_curve(y_test, y_proba)[1::-1])\n",
    "        })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error in iteration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "df_results.to_csv('LOANDEFAULT_RF_Adaptive_Weighting(GSMOTE_NC).csv', index=False)\n",
    "\n",
    "# Display results and feature importances\n",
    "print(df_results.head())\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': range(X_g_smote_imputed.shape[1]),\n",
    "    'importance': weights\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309d6e4-c8c4-4ff9-b977-9de7aadf1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define adaptive weighting function\n",
    "gprime = lambda w: 1. / (2. * np.sqrt(np.abs(w)) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer to data\n",
    "X_S_NC_imputed = imputer.fit_transform(X_S_NC)\n",
    "X_test_sc_imputed = imputer.transform(X_test_sc)\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(1, 101), desc=\"Training Adaptive Random Forest\"):\n",
    "    # Random subsampling (80% of data)\n",
    "    X_train_subset, y_train_subset = resample(\n",
    "        X_S_NC_imputed, y_S_NC,\n",
    "        n_samples=int(0.8 * len(X_S_NC_imputed)),\n",
    "        random_state=i\n",
    "    )\n",
    "    \n",
    "    # Shuffle data\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_subset, y_train_subset, random_state=i)\n",
    "    X_train_array = np.asarray(X_train_shuffled)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = np.ones(X_train_array.shape[1])\n",
    "    \n",
    "    # Adaptive weighting iterations\n",
    "    for _ in range(5):\n",
    "        # Apply weights to features\n",
    "        X_weighted = X_train_array / weights\n",
    "        X_weighted = X_weighted + np.random.normal(0, 1e-3, X_weighted.shape)  # Small jitter\n",
    "        \n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=250,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=i,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_weighted, y_train_shuffled)\n",
    "            \n",
    "            # Update weights using feature importances\n",
    "            weights = gprime(model.feature_importances_)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training error in iteration {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Make predictions\n",
    "    try:\n",
    "        # Apply final weights to test data\n",
    "        X_test_weighted = X_test_sc_imputed / weights\n",
    "        \n",
    "        y_proba = model.predict_proba(X_test_weighted)[:, 1]\n",
    "        threshold = np.random.uniform(0.4, 0.6)\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'iteration': i,\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'threshold': threshold\n",
    "        }\n",
    "        \n",
    "        # Additional metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        metrics.update({\n",
    "            'specificity': tn / (tn + fp),\n",
    "            'g_mean': np.sqrt(metrics['recall'] * (tn / (tn + fp))),\n",
    "            'pr_auc': auc(*precision_recall_curve(y_test, y_proba)[1::-1])\n",
    "        })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error in iteration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "df_results.to_csv('LOANDEFAULT_RF_Adaptive_Weighting(SMOTE_NC).csv', index=False)\n",
    "\n",
    "# Display results and feature importances\n",
    "print(df_results.head())\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': range(X_S_NC_imputed.shape[1]),\n",
    "    'importance': weights\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6ca3f-f3f2-43e6-a3ce-95323c7ee299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
